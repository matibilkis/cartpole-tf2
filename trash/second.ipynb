{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]W1030 13:45:00.939733 140192390350656 base_layer.py:1814] Layer qn_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expand_dims() missing 1 required positional argument: 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-49518eecb6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-49518eecb6b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(primary_network, memory, tarket_network)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mopt_q_tp1_eachS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim_qtp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mq_from_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mupdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_from_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_q_tp1_eachS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expand_dims() missing 1 required positional argument: 'axis'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import keras\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tqdm import tqdm\n",
    "\n",
    "STORE_PATH = '/run'\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "LAMBDA = 0.0005\n",
    "GAMMA = 0.95\n",
    "BATCH_SIZE = 32\n",
    "TAU = 0.08\n",
    "RANDOM_REWARD_STD = 1.0\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "state_size = 4\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "        \n",
    "memory = Memory(50000)\n",
    "\n",
    "\n",
    "class QN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QN,self).__init__()\n",
    "        self.l1 = Dense(30, input_shape=(4,), kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l2 = Dense(30, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "        self.l3 = Dense(num_actions, kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    def call(self, input):\n",
    "        feat = tf.nn.relu(self.l1(input))\n",
    "        feat = tf.nn.relu(self.l2(feat))\n",
    "        value = self.l3(feat)\n",
    "        return value\n",
    "\n",
    "def choose_action(state, primary_network, eps):\n",
    "    if random.random() < eps:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        state = np.expand_dims(np.array(state),axis=0) #otherwise throuhg eerror..\n",
    "        return np.argmax(primary_network(state))\n",
    "\n",
    "\n",
    "def train(primary_network, memory, tarket_network):\n",
    "    if memory.num_samples < BATCH_SIZE*3:\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"doing!!!\")\n",
    "        batch = memory.sample(BATCH_SIZE)\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        actions = np.array([val[1] for val in batch])\n",
    "        rewards = np.array([val[2] for val in batch])\n",
    "        next_states = np.array([(np.zeros(state_size)\n",
    "                                 if val[3] is None else val[3]) for val in batch])\n",
    "\n",
    "        prim_qt = primary_network(np.expand_dims(states,axis=0)) # Q_t[s,a]\n",
    "        prim_qtp1 = primary_network(np.expand_dims(next_states,axis=0)) #Q_{t+1}[s,a]\n",
    "\n",
    "        updates = rewards\n",
    "        valid_idxs = np.array(next_states).sum(axis=1) != 0\n",
    "        batch_idxs = np.arange(BATCH_SIZE)\n",
    "\n",
    "        opt_q_tp1_eachS = np.argmax(np.squeeze(prim_qtp1.numpy()), axis=0)\n",
    "        q_from_target = target_network(np.expand_dims(next_states), axis=0)\n",
    "\n",
    "        updates[valid_idx] += GAMMA*np.squeeze(q_from_target.numpy())[valid_idx, opt_q_tp1_eachS[valid_idx]]\n",
    "\n",
    "        target_q = np.squeeze(prim_qt.numpy())\n",
    "        target_q[batch_idxs, actions] = updates\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(primary_network.trainable_variables)\n",
    "                predicted_q = primary_network(states)\n",
    "                target_q = np.expand_dims(target_q,axis=0)\n",
    "                loss = tf.keras.losses.MSE(predicted_q, target_q)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                print(loss)\n",
    "                grads = tape.gradient(loss, primary_network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, primary_network.trainable_variables))\n",
    "\n",
    "        for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "            t.assign(t*(1-TAU) + e*TAU)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "primary_network = QN()\n",
    "target_network = QN()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_episodes  = 10**4\n",
    "eps = MAX_EPSILON\n",
    "render = False\n",
    "# train_writer = tf.summary.create_file_writer(\"summarie/\"+f\"/DoubleQ_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "steps = 0\n",
    "\n",
    "for i in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    cnt=0\n",
    "    avg_loss=0\n",
    "    while True:\n",
    "        action = choose_action(state, primary_network, eps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = np.random.normal(1.0, RANDOM_REWARD_STD)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        memory.add_sample((state, action, reward, next_state))\n",
    "        loss = train(primary_network, memory, target_network)\n",
    "        avg_loss += loss\n",
    "        state = next_state\n",
    "        steps +=1\n",
    "        eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON)*np.exp(- LAMBDA*steps)\n",
    "        if done:\n",
    "            avg_loss /= cnt\n",
    "            # print(f\"Episode: {i}, Reward: {cnt}, avg loss: {avg_loss:.3f}, eps: {eps:.3f}\")\n",
    "            # with train_writer.as_default():\n",
    "            #     tf.summary.scalar('reward', cnt, step=i)\n",
    "            #     tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "            break\n",
    "        cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = QN()\n",
    "target_network = QN()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing!!!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expand_dims() missing 1 required positional argument: 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2face083b7b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-49518eecb6b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(primary_network, memory, tarket_network)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mopt_q_tp1_eachS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim_qtp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mq_from_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mupdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_from_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_q_tp1_eachS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expand_dims() missing 1 required positional argument: 'axis'"
     ]
    }
   ],
   "source": [
    "num_episodes  = 10**4\n",
    "eps = MAX_EPSILON\n",
    "render = False\n",
    "train_writer = tf.summary.create_file_writer(\"summarie/\"+f\"/DoubleQ_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "steps = 0\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    cnt=0\n",
    "    avg_loss=0\n",
    "    while True:\n",
    "        action = choose_action(state, primary_network, eps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = np.random.normal(1.0, RANDOM_REWARD_STD)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        memory.add_sample((state, action, reward, next_state))\n",
    "        loss = train(primary_network, memory, target_network)\n",
    "        avg_loss += loss\n",
    "        state = next_state\n",
    "        steps +=1\n",
    "        eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON)*np.exp(- LAMBDA*steps)\n",
    "        if done:\n",
    "            avg_loss /= cnt\n",
    "            print(f\"Episode: {i}, Reward: {cnt}, avg loss: {avg_loss:.3f}, eps: {eps:.3f}\")\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('reward', cnt, step=i)\n",
    "                tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "            break\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
